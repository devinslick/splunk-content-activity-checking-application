# Search macros for CACA - Content Activity Checking Application

#####################
# Query Macros
#####################

[get_dashboard_stats(1)]
args = dashboard_name
definition = | mstats sum(_value) as metric_value WHERE index=caca_metrics AND pretty_name="$dashboard_name$" BY pretty_name, metric_name, activity_type, app span=1h \
| eval metric_display=case(\
    metric_name=="dashboard.views", "Views",\
    metric_name=="dashboard.edits", "Edits",\
    metric_name=="dashboard.errors", "Errors",\
    metric_name=="dashboard.load_time", "Load Time (ms)",\
    1=1, metric_name) \
| eval _time=_time
iseval = 0

[get_dashboard_stats_timerange(2)]
args = dashboard_name, timerange
definition = | mstats sum(_value) as metric_value WHERE index=caca_metrics AND pretty_name="$dashboard_name$" BY pretty_name, metric_name, activity_type, app span=1h \
| where _time >= relative_time(now(), "$timerange$") \
| eval metric_display=case(\
    metric_name=="dashboard.views", "Views",\
    metric_name=="dashboard.edits", "Edits",\
    metric_name=="dashboard.errors", "Errors",\
    metric_name=="dashboard.load_time", "Load Time (ms)",\
    1=1, metric_name) \
| stats sum(metric_value) as total_value by pretty_name, metric_display, metric_name \
| eval timerange="$timerange$"
iseval = 0

[get_all_dashboards_summary]
definition = | mstats sum(_value) as metric_value WHERE index=caca_metrics BY pretty_name, metric_name, app span=1d \
| where _time >= relative_time(now(), "-7d") \
| stats sum(metric_value) as total_value by pretty_name, metric_name, app \
| eval metric_type=case(\
    metric_name=="dashboard.views", "views_7d",\
    metric_name=="dashboard.edits", "edits_7d",\
    metric_name=="dashboard.errors", "errors_7d",\
    metric_name=="dashboard.load_time", "load_time_7d",\
    1=1, "other") \
| eval {metric_type}=total_value \
| stats values(app) as app sum(views_7d) as views_7d sum(edits_7d) as edits_7d sum(errors_7d) as errors_7d avg(load_time_7d) as avg_load_time_7d by pretty_name \
| fillnull value=0 views_7d edits_7d errors_7d avg_load_time_7d \
| eval health_status=case(\
    errors_7d > 10, "critical",\
    errors_7d > 0, "warning",\
    views_7d == 0, "stale",\
    1=1, "healthy")
iseval = 0

[get_dashboard_last_viewed(1)]
args = dashboard_name
definition = | mstats latest(_value) as last_view WHERE index=caca_metrics AND pretty_name="$dashboard_name$" AND metric_name="dashboard.views" BY pretty_name \
| eval last_viewed=strftime(_time, "%Y-%m-%d %H:%M:%S") \
| eval days_since_view=round((now()-_time)/86400, 1)
iseval = 0

[get_top_dashboards(1)]
args = metric_type
definition = | mstats sum(_value) as metric_value WHERE index=caca_metrics AND metric_name="dashboard.$metric_type$" BY pretty_name, app span=1d \
| where _time >= relative_time(now(), "-7d") \
| stats sum(metric_value) as total by pretty_name, app \
| sort -total \
| head 10 \
| eval rank=row_number() \
| eval metric_type="$metric_type$"
iseval = 0

[get_dashboard_performance(1)]
args = dashboard_name
definition = | mstats avg(_value) as avg_load_time WHERE index=caca_metrics AND pretty_name="$dashboard_name$" AND metric_name="dashboard.load_time" BY pretty_name span=1h \
| eval avg_load_time=round(avg_load_time, 2) \
| eval performance_rating=case(\
    avg_load_time &lt; 1000, "Excellent",\
    avg_load_time &lt; 3000, "Good",\
    avg_load_time &lt; 5000, "Fair",\
    1=1, "Poor")
iseval = 0

[get_slow_dashboards]
definition = | mstats avg(_value) as avg_load_time WHERE index=caca_metrics AND metric_name="dashboard.load_time" BY pretty_name, app span=1d \
| where _time >= relative_time(now(), "-7d") \
| stats avg(avg_load_time) as avg_load_time_7d by pretty_name, app \
| where avg_load_time_7d &gt; 3000 \
| eval avg_load_time_7d=round(avg_load_time_7d, 0) \
| eval performance_status=case(\
    avg_load_time_7d &gt; 10000, "Critical",\
    avg_load_time_7d &gt; 5000, "Poor",\
    avg_load_time_7d &gt; 3000, "Fair",\
    1=1, "Good") \
| sort -avg_load_time_7d
iseval = 0

[get_dashboards_with_errors]
definition = | mstats sum(_value) as error_count WHERE index=caca_metrics AND metric_name="dashboard.errors" BY pretty_name, severity, app span=1d \
| where _time >= relative_time(now(), "-7d") \
| stats sum(error_count) as total_errors by pretty_name, severity, app \
| eval {severity}=total_errors \
| stats sum(error) as errors sum(warn) as warnings values(error) as has_errors values(warn) as has_warns sum(total_errors) as total_issues by pretty_name, app \
| fillnull value=0 errors warnings \
| where total_issues &gt; 0 \
| eval health_status=case(\
    errors &gt; 50, "Critical",\
    errors &gt; 10, "High",\
    errors &gt; 0, "Medium",\
    warnings &gt; 20, "Medium",\
    1=1, "Low") \
| sort -errors -warnings
iseval = 0

[get_problematic_dashboards]
definition = `get_all_dashboards_summary` \
| where health_status="critical" OR health_status="warning" OR avg_load_time_7d &gt; 5000 \
| eval issue_type=case(\
    health_status="critical" AND avg_load_time_7d &gt; 5000, "Health + Performance",\
    health_status="critical", "Health Issues",\
    health_status="warning" AND avg_load_time_7d &gt; 5000, "Health + Performance",\
    health_status="warning", "Health Issues",\
    avg_load_time_7d &gt; 5000, "Performance Issues",\
    1=1, "Other") \
| table pretty_name app views_7d errors_7d avg_load_time_7d health_status issue_type \
| sort -errors_7d -avg_load_time_7d
iseval = 0

[filter_by_app]
definition = lookup app_filter app OUTPUT include \
| where isnull(include) OR include="true" OR include="1" OR include="yes" \
| fields - include
iseval = 0
